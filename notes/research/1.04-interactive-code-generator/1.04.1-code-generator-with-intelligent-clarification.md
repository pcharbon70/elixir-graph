# Building an Interactive Elixir Code Generator with Intelligent Clarification

**Single-round clarification transforms code generation from guesswork into precision.** When a user asks for "a function to process orders," does that mean validation, transformation, or database persistence? Should failures raise exceptions or return error tuples? The difference between generating correct code and generating code that requires extensive debugging often comes down to one well-chosen question asked before generation begins.

This report presents a comprehensive framework for implementing clarification-seeking behavior in an Elixir code generation model built with Nx and Axon. The approach integrates with existing ontology-augmented pipelines, leverages Credo and Sobelow rules to identify ambiguities, and uses uncertainty quantification to decide when asking a question will improve the final code. Research from ClarifyGPT, CodeT5, and semantic entropy methods provides the foundation, while Elixir-specific patterns inform the practical implementation.

---

## Section 1: Detecting ambiguous task requirements

Requirement ambiguity manifests when multiple valid code implementations could satisfy a natural language specification. The SpecFix framework from ICSE 2025 introduced a powerful detection method: **generate multiple code samples from the same prompt at elevated temperature and measure their divergence**. When generated solutions cluster into distinct functional behaviors, the underlying requirement contains genuine ambiguity.

The ClarifyGPT framework operationalizes this through a code consistency check. Given a prompt, the system generates five solutions with temperature 0.8, then clusters them by AST structure or behavioral equivalence on test inputs. High cluster counts signal ambiguity requiring clarification. This approach dramatically outperforms directly prompting models to identify ambiguity—research shows LLMs struggle with metacognition about their own uncertainty without this multi-sample grounding.

**Elixir-specific ambiguity patterns** require targeted detection. The language's emphasis on pattern matching means missing case clauses represent a critical ambiguity type. When prompts mention data processing without specifying empty list behavior, nil handling, or invalid type responses, the model should generate clarifying questions like "Should empty lists return nil, an empty list, or raise an error?" Guard clause scope presents another Elixir-specific concern—numeric constraints without explicit bounds warrant questions about acceptable ranges.

The distinction between **genuine ambiguity and implicit conventions** requires careful handling. Elixir has strong conventions: fallible operations return `{:ok, result}` or `{:error, reason}` tuples; GenServer `start_link` accepts keyword options; Phoenix controllers return `conn`. The model should apply these conventions by default, only asking when conventions conflict with explicit requirements or when multiple conventions could apply.

A practical detection algorithm combines syntactic divergence analysis (different control flow structures across samples), semantic divergence (different behavior on generated test inputs), and type signature divergence (variance in inferred input/output types). Thresholds on these metrics trigger the clarification pathway rather than direct generation.

---

## Section 2: When design and style choices demand clarification

Elixir's rich concurrency model and OTP patterns create frequent decision points where multiple valid approaches exist. The choice between GenServer, Agent, Task, and raw processes fundamentally affects code structure, yet prompts rarely specify which pattern to use.

**OTP pattern detection** examines linguistic signals in prompts. References to "storing state" or "keeping track of" suggest Agent or GenServer, with GenServer preferred for complex synchronization. Mentions of "async operations" or "one-off tasks" point toward Task. When prompts contain "handle requests" or "process messages," GenServer becomes appropriate. The critical clarification trigger occurs when signals are mixed or absent—asking "Should this maintain state between calls (GenServer/Agent) or handle one-off async operations (Task)?" resolves architectural uncertainty early.

Style variations in Elixir present another clarification category. The choice between `with` and `case` statements affects error handling flow—`with` short-circuits on failure while `case` allows exhaustive matching. When prompts involve chained fallible operations, asking about preferred error handling style prevents generating idiomatically wrong code. Pipe chains versus nested function calls represent aesthetic preferences that experienced developers hold strong opinions about.

**Library choice points** emerge when prompts reference database operations ("persist," "query," "schema") without specifying whether Ecto is in use. The question "Is Ecto already in use, or should this use raw SQL?" prevents generating incompatible code. Similarly, HTTP endpoint mentions trigger Phoenix versus Plug clarification when context doesn't already specify the framework.

Design trade-off presentation should be **concrete rather than abstract**. Instead of asking "What GenServer callback strategy should be employed?" the model should ask "Should the function restart automatically if it fails, or report failures to a supervisor for handling?" Providing brief examples in the question—"`restart: :transient` vs `:permanent`"—helps users make informed decisions without requiring deep OTP knowledge.

---

## Section 3: Identifying missing domain context

The third clarification category addresses missing information that cannot be inferred from conventions or the prompt itself. When prompts reference "the user's email" or "validate the order," they assume domain context the model lacks.

**Slot-filling approaches** adapted from task-oriented dialogue systems provide a structured detection method. Define a schema of required information for code generation tasks:

- **Function identity**: name, module location, arity
- **Type specifications**: input types, output type, type parameters  
- **Error handling**: exception vs. tuple returns, specific error atoms
- **External references**: schema fields, module signatures, API endpoints
- **Business logic**: validation rules, workflows, permissions

When prompts mention entities without definitions—"the User schema," "the validation module," "the external API"—these represent missing slots requiring clarification. The question "What fields does the User schema contain?" is more actionable than proceeding with assumed fields.

**Integration context** requires particular attention. References to database tables, API endpoints, or external services without specifications create high-risk ambiguity. Questions like "What is the API endpoint URL pattern and authentication method?" prevent generating non-functional integration code.

The **reverse thinking method** works backward from solution requirements. Rather than analyzing the prompt forward, consider what output is needed, what data transformations that requires, and what failure modes exist. This often reveals missing prerequisites not obvious from forward analysis. If generating an order processing function, working backward reveals the need for order schema structure, valid state transitions, and failure handling—information often missing from initial prompts.

Distinguishing inferable from truly missing context relies on Elixir ecosystem knowledge. Common struct patterns (Ecto schemas, Phoenix contexts) can be reasonably assumed. Business domain rules, external system interfaces, and non-standard error codes cannot be inferred and require explicit clarification.

---

## Section 4: Uncertainty quantification for the ask-or-proceed decision

The central technical challenge involves determining **when uncertainty is high enough to justify asking a question** versus proceeding with generation. This requires quantifying model uncertainty and calibrating thresholds appropriately.

**Semantic entropy** provides the most effective uncertainty measure for code generation. Token-level entropy (average prediction entropy across generated tokens) captures lexical uncertainty but misses semantic equivalence—two functionally identical programs may have very different token sequences. Semantic entropy instead clusters generated samples by meaning equivalence, then computes entropy over cluster probabilities. For code, "meaning equivalence" translates to functional equivalence on test inputs: samples that produce identical outputs on identical inputs belong to the same semantic class.

The mathematical foundation combines predictive entropy with clustering:

```
Semantic Entropy = -Σ P(cluster_i) × log₂(P(cluster_i))
```

Low semantic entropy (single cluster, all samples functionally equivalent) indicates confidence—proceed with generation. High semantic entropy (three or more distinct behavioral clusters) indicates genuine ambiguity—ask a clarifying question. Medium entropy cases require secondary signals.

**Calibration matters critically** for threshold setting. Research from ICSE 2025 demonstrates that code-generating LLMs are poorly calibrated out of the box—their confidence scores don't align with actual correctness rates. Platt scaling provides a correction: `calibrated_prob = sigmoid(a × logit + b)` where parameters `(a, b)` are fit on a held-out validation set with ground truth correctness labels. This requires ~100-500 labeled examples but dramatically improves threshold reliability.

The **decision-theoretic framework** balances costs:

- Correct code generation: utility +1.0
- Incorrect code requiring debugging: utility -0.5  
- Asking a clarifying question: utility -0.1 (user friction)
- Clarification leading to correct code: utility +0.9

Under this framework, the expected utility of asking exceeds proceeding when `(1 - p_correct) × 0.5 > 0.1 + 0.1 × (1 - p_improvement)`, which simplifies to asking when confidence falls below approximately **0.7** and a high-quality question is available.

**User fatigue mitigation** constrains asking frequency. Rate limiting to one clarification per 3-5 requests, minimum expected information gain thresholds, and tracking whether users provide helpful answers all prevent over-asking. The single-round paradigm helps—users tolerate one well-chosen question far better than multi-turn interrogation.

---

## Section 5: Generating high-quality clarifying questions

Question generation requires producing **specific, actionable questions** that will genuinely improve the subsequent code generation. The Expected Value of Perfect Information (EVPI) framework from Rao & Daumé provides the theoretical foundation: a good question is one whose expected answer will be useful.

The mathematical formulation scores questions by:

```
EVPI(question | prompt) = Σ P(answer | prompt, question) × Utility(prompt + answer)
```

This requires estimating the probability distribution over possible answers and the utility of conditioning generation on each answer. For code generation, utility correlates with the reduction in code divergence—a question whose answers would collapse five distinct behavioral clusters into one provides high value.

**Template-based approaches** offer controlled quality for common patterns. Elixir-specific templates include:

- **Error handling**: "How should {function_name} handle errors? A) Return {:ok, result}/{:error, reason} tuples B) Raise exceptions C) Return nil for failures"
- **Concurrency model**: "Will {module_name} need to maintain state between calls? A) Yes—use GenServer B) No—use simple functions C) Yes, read-only—use Agent"
- **Pattern completeness**: "What should happen when {function_name} receives {edge_case}? A) Return default value B) Raise ArgumentError C) Return {:error, :invalid_input}"

Templates ensure consistent quality but lack flexibility for novel situations. **Hybrid approaches** use templates for recognized ambiguity types while generating novel questions for unrecognized patterns.

**Question quality metrics** evaluate candidates on multiple dimensions:

- **Specificity**: Does the question target the exact ambiguity? (Generic "please clarify" questions score low)
- **Answerability**: Can users reasonably answer without extensive research?
- **Informativeness**: Will the answer actually resolve the ambiguity?
- **Efficiency**: Is this the minimum information needed?

**Diversity filtering** prevents redundant questions when multiple clarifications are needed. Maximum Marginal Relevance balances relevance with diversity—each subsequent question should cover different aspects of the ambiguity. Semantic similarity thresholds (removing questions with >0.8 cosine similarity) provide a simpler alternative.

Phrasing technical questions accessibly requires **concrete examples over abstract terminology**. Instead of "What GenServer callback semantics should be employed?" ask "Should this process restart automatically after crashes, or should errors be reported to a supervisor?" Include brief code examples when they clarify options.

---

## Section 6: Integrating answers into generation

Once users provide clarification answers, the model must condition subsequent generation on this new context. The core challenge involves ensuring generated code actually reflects the clarification rather than reverting to default patterns.

**Cross-attention mechanisms** provide the fundamental architectural approach in encoder-decoder models. The decoder's cross-attention layer allows each generation step to attend to the encoded clarification context, selectively focusing on relevant constraints. Research identifies an "attention degeneration" problem in decoder-only models: attention to source context decreases as generation length increases, causing later tokens to ignore clarifications. Encoder-decoder architectures with explicit cross-attention mitigate this issue.

**Prompt engineering for answer incorporation** follows structured templates:

```
Task: [Original user request]

Clarification Q1: [Question asked]
Answer A1: [User's response]

Clarification Q2: [Question asked]  
Answer A2: [User's response]

---
Generate Elixir code that satisfies the above requirements.
Your implementation MUST reflect the specific behaviors described in the answers.
```

Positioning clarification answers **close to the generation instruction** improves attention—the "lost in the middle" phenomenon causes models to attend more strongly to context at sequence boundaries.

**Chain of Grounded Objectives (CGO)** embeds functional requirements as comment-level representations within the code:

```elixir
# Requirement: Handle nil inputs by returning {:error, :nil_input}
# Requirement: Use GenServer for stateful processing
# Requirement: Log all state transitions via Logger.info
```

This approach achieves higher accuracy than Chain-of-Thought prompting while using fewer tokens, as models trained on code are accustomed to comment-style objectives.

**Handling imperfect answers** requires graceful degradation. When answers are ambiguous, generate code with explicit documented assumptions. When answers are incomplete, implement clarified portions fully while marking unclarified aspects with TODO comments. When answers conflict, prefer later answers (recency) or more specific answers (specificity), and flag conflicts in generated comments.

**Verification that answers were used** employs attention analysis (checking that clarification tokens receive significant attention throughout generation) and post-hoc NLI checking (verifying generated code doesn't contradict stated constraints). Requiring inline citations in generated comments—`# Per clarification [1]: using GenServer pattern`—enables explicit traceability.

---

## Section 7: Architecture and training for clarification capability

Building clarification capability from scratch requires careful architectural decisions and multi-phase training strategies.

**Encoder-decoder architecture** is recommended over decoder-only for clarification-capable models. The encoder processes user queries with bidirectional attention, enabling full comprehension of ambiguity before generation begins. The decoder then generates either clarification questions or code depending on the detection result. CodeT5's design demonstrates this pattern: a **shallow encoder with deep decoder** balances comprehension and generation capabilities efficiently.

**Multi-task architecture with hard parameter sharing** addresses the three core tasks:

```
Input Query → [Shared Encoder] →
    ├── [Detection Head] → Binary: Ask/Proceed  
    ├── [Question Generator] → Clarification question (if ambiguous)
    └── [Code Generator] → Elixir code (if clear)
```

Shared layers reduce overfitting risk proportionally to the number of tasks while task-specific output heads preserve specialization. Joint training uses weighted loss: `L_total = α×L_detection + β×L_question + γ×L_code`.

**Curriculum learning** orders training examples by difficulty. Begin with clearly-specified requirements where the model learns basic code generation. Progress to mildly ambiguous cases where multiple valid solutions exist. Finally, train on highly ambiguous prompts requiring clarification. Automated curriculum selection using learning progress signals (prediction gain, gradient magnitude) adapts difficulty dynamically.

**Training the ask-or-proceed decision** uses binary classification on ambiguity labels. The code consistency check method provides weak supervision: generate multiple solutions for each training prompt, cluster by functional equivalence, and label prompts with high cluster counts as "needs clarification." This avoids expensive manual annotation while providing meaningful signal.

**RLHF for question quality** fine-tunes the question generator based on human preferences. The reward model scores questions on helpfulness, specificity, and relevance—dimensions where automated metrics correlate poorly with human judgment. PPO optimization with KL penalty prevents mode collapse while improving question quality. For code, execution-based feedback supplements human feedback: questions leading to higher pass@1 rates receive positive reward.

**Datasets** combine existing resources with synthetic generation. CodeSearchNet provides 2M code-comment pairs across six languages (though Elixir-specific data requires curation from Hex.pm and GitHub). Synthetic clarification data uses LLM-based generation: take existing Elixir code, generate ambiguous versions of requirements, then generate corresponding clarification questions and refined requirements. Mining GitHub PR comments and Stack Overflow threads for clarification patterns provides additional naturalistic data.

---

## Section 8: Practical implementation in Elixir Nx/Axon

Implementing this framework in Elixir Nx/Axon requires adapting the research findings to the ecosystem's capabilities and constraints.

**Model architecture in Axon** builds a shared transformer encoder with task-specific decoder heads:

```elixir
defmodule ClarifyElixir.Model do
  import Axon

  def build(config) do
    # Shared encoder with multi-head self-attention
    encoder = build_encoder(config)
    
    # Task routing based on detection
    detection_head = 
      encoder
      |> dense(config.hidden_size, activation: :gelu)
      |> dense(2, activation: :softmax, name: "should_ask")
    
    question_decoder = build_decoder(encoder, config, "question")
    code_decoder = build_decoder(encoder, config, "code")
    
    {encoder, detection_head, question_decoder, code_decoder}
  end
  
  defp build_encoder(config) do
    input({nil, config.max_seq_length}, name: "input_ids")
    |> embedding(config.vocab_size, config.hidden_size)
    |> transformer_blocks(config.num_layers, config.num_heads)
  end
end
```

**Integration with the existing ontology-augmented pipeline** treats clarification as a preprocessing step. Before generating code, the clarification module:

1. Extracts Elixir-specific signals from the prompt using ontology annotations
2. Generates multiple code sketches to assess divergence
3. Consults Credo rules to identify potential style ambiguities
4. Runs Sobelow patterns to detect security-relevant missing specifications
5. Triggers clarification if uncertainty exceeds threshold

**Using Credo and Sobelow for ambiguity detection** leverages existing static analysis. Credo rules about consistency (readability, refactoring opportunities) map to style clarification triggers. Sobelow's security checks (unsafe deserialization, SQL injection, missing authentication) identify domain context that must be explicit. When prompts reference external data without sanitization specifications, the model asks about input validation requirements.

**Single-round interaction flow**:

```elixir
defmodule ClarifyElixir.Pipeline do
  def generate(prompt, opts \\ []) do
    with {:ok, analysis} <- analyze_prompt(prompt),
         {:proceed, _} <- check_clarification_needed(analysis) do
      # Clear prompt - generate directly
      generate_code(prompt, analysis)
    else
      {:ask, questions} ->
        # Ambiguous - return top question
        {:clarification_needed, select_best_question(questions)}
    end
  end
  
  def generate_with_clarification(prompt, question, answer) do
    augmented_prompt = build_augmented_prompt(prompt, question, answer)
    analysis = analyze_prompt(augmented_prompt)
    generate_code(augmented_prompt, analysis)
  end
end
```

**State management between phases** stores the original prompt, detected ambiguities, and generated questions in the session. After receiving clarification, the pipeline reconstructs the augmented prompt and proceeds to generation without re-analyzing for clarification (single-round constraint).

**Performance optimization** for interactive use caches encoder representations of the original prompt. When clarification arrives, only the answer encoding and decoder generation require computation. Quantization of the model weights using Nx's low-precision support (FP16 or INT8) reduces memory and accelerates inference.

---

## Section 9: Evaluating clarification quality and impact

Comprehensive evaluation spans clarification detection, question quality, and downstream code improvement.

**Detection metrics** assess the ask-or-proceed decision:

- **Precision**: Of prompts flagged as ambiguous, what fraction genuinely required clarification?
- **Recall**: Of genuinely ambiguous prompts, what fraction were correctly detected?
- **Specificity**: Correctly identifying clear prompts that don't need clarification

The ground truth requires human annotation of ambiguity—a labeling task where moderate inter-annotator agreement (~0.7 Cohen's κ) is expected given ambiguity's inherent subjectivity.

**Question quality metrics** evaluate generated questions:

- **Relevance**: Does the question address the actual ambiguity? (NLI-based scoring against detected ambiguity)
- **Specificity**: Is the question targeted vs. generic? (Length and detail metrics)
- **Answerability**: Can users answer without extensive research? (Human evaluation)
- **Informativeness**: Does the answer actually reduce code divergence? (Before/after clustering comparison)

**Code quality improvement** measures whether clarification helps:

- **ΔPass@1**: Improvement in test pass rate after clarification versus direct generation
- **Code consistency**: Reduction in variance across generated solutions after clarification
- **Semantic similarity**: Embedding similarity between generated code and ground truth reference

ClarifyGPT achieved **62.43% → 69.60% pass rate** improvement on standard benchmarks, demonstrating substantial gains from well-designed clarification.

**Human evaluation protocols** use structured rubrics:

| Criterion | Score 1-5 | Description |
|-----------|-----------|-------------|
| Addresses core ambiguity | 2 points | Question targets the main uncertainty |
| Sufficiently specific | 1 point | Not generic "please clarify" |
| Avoids unnecessary asking | 1 point | Question isn't obvious from context |
| Natural phrasing | 1 point | Grammatically fluent and clear |

**A/B testing framework** compares task completion with and without clarification capability. Developers perform coding tasks using both model versions. Metrics include time to acceptable code, revision count required, and satisfaction ratings. The single-round constraint enables clean comparison—users receive either zero or one clarification per task.

**Automated evaluation limitations** require acknowledgment. BLEU and ROUGE correlate weakly with human judgment for dialogue quality. For code, CodeBLEU (including AST and dataflow matching) outperforms token-overlap metrics. Execution-based evaluation (running generated code against test suites) provides the most reliable automated signal, though it requires test coverage for evaluation prompts.

---

## Conclusion: A practical path to intelligent clarification

Implementing clarification-seeking behavior in an Elixir code generator requires integrating multiple research threads: multi-sample divergence detection identifies when clarification would help, semantic entropy quantifies uncertainty for threshold decisions, EVPI-based ranking selects high-value questions, and cross-attention conditioning ensures answers improve generation.

The **key architectural insight** is treating clarification as a multi-task learning problem with shared representations. A single encoder-decoder model with task-specific heads for detection, question generation, and code generation leverages synergies while enabling end-to-end training.

**Elixir-specific opportunities** include leveraging the language's strong conventions to reduce unnecessary questions, using Credo rules as ambiguity signals, and generating questions about OTP patterns that reflect the language's concurrency model. The ontology-augmented pipeline provides semantic structure for detecting missing context.

For the single-round paradigm, **question selection becomes critical**. With only one question allowed, the model must identify the highest-value clarification—typically the one that collapses the most behavioral clusters or fills the most consequential missing slot. Expected information gain ranking, combined with Elixir-specific templates for common patterns, balances quality with coverage.

The evaluation framework must capture not just whether clarification occurs but whether it improves outcomes. ΔPass@1 provides the north-star metric: does asking one question before generating yield meaningfully better code than generating directly? Research suggests improvements of 5-10 percentage points are achievable, making the small friction cost of one clarifying question worthwhile when uncertainty is genuinely high.
