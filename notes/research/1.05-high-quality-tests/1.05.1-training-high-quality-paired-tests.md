# Training an Elixir code-test LLM from scratch with Nx/Axon

Building a language model that generates high-quality Elixir tests alongside code requires integrating mutation testing signals, domain-specific tokenization, and test-aware pre-training objectives. This guide synthesizes the complete technical approach using **Muzak** for execution feedback, **Lorax** for efficient task adaptation, and the **Nx/Axon** ecosystem for pure-Elixir training infrastructure.

The recommended architecture is an encoder-decoder transformer (CodeT5-style) in the **125M-350M parameter range**, trained on paired code-test data from Hex.pm and GitHub, with Muzak mutation scores providing execution feedback during reinforcement learning stages. After pre-training, Lorax enables efficient adaptation for specialized test types—ExUnit, StreamData property tests, and LiveViewTest—without retraining the entire model.

---

## Muzak powers mutation-based test quality signals

**Muzak** is Devon Estes' mutation testing library for Elixir, available in open-source (limited to **1,000 mutations per run**) and Pro versions ($29/month with unlimited mutations and parallel execution). The library generates mutants by modifying source code AST nodes, then executes your test suite against each mutant—tests that fail to catch mutations reveal gaps in coverage.

### Supported mutation operators

Muzak transforms code through several operator categories, though the open-source documentation doesn't provide an exhaustive list:

| Mutation Type | Original | Mutated |
|--------------|----------|---------|
| Arithmetic operators | `num + 1` | `num - 1` |
| Membership operators | `num in [2,3,5]` | `num not in [2,3,5]` |
| Integer literals | `num + 1` | `num + 0` |
| Boolean replacement | `if condition` | `if true` |
| List modification | `[2, 3, 5, 8]` | `[11, 3, 19]` |
| Atom replacement | `[:admin, :owner]` | `[:admin, :random]` |
| Function renaming | `def maybe_add` | `def daa_ebyam` |

**Muzak Pro** adds over a dozen additional mutators including pattern matching mutations and guard clause modifications. The Enterprise tier enables custom mutator development—potentially valuable for Elixir-specific constructs like pipe chains and with clauses.

### Programmatic integration for training pipelines

Muzak lacks a documented public API for programmatic execution—it's designed as a Mix task. For training pipeline integration, invoke it through Mix:

```elixir
defmodule MuzakRunner do
  def run_mutation_testing(module_path, opts \\ []) do
    mutations = Keyword.get(opts, :mutations, 100)
    seed = Keyword.get(opts, :seed, :rand.uniform(1_000_000))
    
    # Execute via Mix task
    {output, exit_code} = System.cmd("mix", [
      "muzak",
      "--mutations", Integer.to_string(mutations),
      "--seed", Integer.to_string(seed),
      "--only", module_path
    ], stderr_to_stdout: true)
    
    parse_mutation_results(output, exit_code)
  end
  
  defp parse_mutation_results(output, exit_code) do
    # Extract mutation score from CLI output
    # Format: "25 mutations run - 23 mutations survived"
    %{
      passed: exit_code == 0,
      raw_output: output,
      score: extract_score(output)
    }
  end
end
```

Configuration profiles in `.muzak.exs` enable CI integration with minimum coverage thresholds:

```elixir
%{
  training: [
    min_coverage: 80.0,
    mutation_filter: fn files ->
      files
      |> Enum.filter(&String.ends_with?(&1, ".ex"))
      |> Enum.reject(&String.starts_with?(&1, "test/"))
      |> Enum.map(&{&1, nil})
    end
  ]
}
```

### Performance characteristics for training loops

Running Muzak during training creates computational challenges. Each mutation requires **recompiling the mutated file plus dependencies** and executing relevant tests. Real-world performance ranges from ~4 seconds per mutation for simple modules to 30+ minutes for projects with extensive property-based tests.

**Optimization strategies for training integration:**

- Cache mutation results by content hash—identical code produces identical mutants
- Batch test execution by running multiple mutants against shared test compilation
- Use Muzak Pro's changed-lines-only mode for incremental feedback
- Sample mutations randomly rather than exhaustively during early training

**Comparison to other ecosystems:** Unlike Stryker (JavaScript) or PIT (Java), Muzak operates at the AST level rather than bytecode, requiring full recompilation. This is slower but enables Elixir-specific mutations on macros and pattern matching. Muzak lacks Stryker's HTML dashboard integration and JSON report formats, limiting automated pipeline integration.

---

## Lorax enables efficient LoRA adaptation for test specialization

**Lorax** implements Low-Rank Adaptation for Axon models, enabling fine-tuning of large models with **dramatically reduced parameter counts**. Created by Ted Wong during Spawnfest, it's the only LoRA implementation in the Elixir ecosystem. Version 0.2.1 (May 2024) integrates seamlessly with Bumblebee models.

### How LoRA works in Axon context

LoRA freezes base model weights and injects trainable low-rank matrices (A and B) into attention projections. Instead of updating full weight matrix W, the output becomes `Wx + BAx` where rank(BA) << rank(W). This reduces trainable parameters from billions to millions while preserving base model capabilities.

Lorax manipulates the Axon computation graph by:

1. Creating a dummy node after target layers
2. Injecting LoRA computation nodes with inputs from original layer
3. Swapping node IDs so downstream layers receive modified outputs

### Core API for applying LoRA adapters

```elixir
# Inject LoRA into a Bumblebee model
{:ok, model_info} = Bumblebee.load_model({:hf, "gpt2"})
%{model: base_model, params: base_params} = model_info

lora_model =
  base_model
  |> Axon.freeze()
  |> Lorax.inject(%Lorax.Config{
    r: 4,           # Rank of adaptation matrices
    alpha: 8,       # Scaling factor (typically 2*r)
    dropout: 0.05,  # Regularization
    target_key: true,
    target_query: true,
    target_value: true
  })
```

For targeting custom layers beyond Q/K/V projections (useful for test generation heads):

```elixir
lora_model =
  base_model
  |> Axon.freeze()
  |> Lorax.inject(%Lorax.Config{
    r: 4,
    alpha: 8,
    target_node_fn: fn %Axon.Node{name: name_fn} ->
      name = name_fn.(nil, nil)
      String.contains?(name, "output") or String.contains?(name, "test_head")
    end
  })
```

### Training and saving LoRA adapters

```elixir
# Training loop with LoRA
{init_fn, predict_fn} = Axon.build(lora_model, mode: :train)

merged_params =
  {init_fn, predict_fn}
  |> Axon.Loop.trainer(custom_loss, Polaris.Optimizers.adam(learning_rate: 3.0e-4))
  |> Axon.Loop.run(train_stream, base_params, 
      epochs: 3, 
      iterations: 1000, 
      compiler: EXLA)

# Extract only LoRA params (~4MB vs GB for full model)
lora_only = Lorax.Params.filter(merged_params, base_params)
File.write!("exunit_lora.lorax", Nx.serialize(lora_only))
```

### When LoRA matters for from-scratch training

Even when training from scratch, LoRA becomes valuable for **task-specific adaptation after pre-training**. Train the base model on general Elixir code-test pairs, then create specialized adapters:

- **ExUnit adapter**: Fine-tuned on unit test patterns with `assert`, `refute`, `assert_raise`
- **StreamData adapter**: Specialized for property-based tests with generators and `check all`
- **LiveViewTest adapter**: Tuned for `render_component`, `live_patch`, and assertion patterns

This approach enables **hot-swapping test generation strategies** without maintaining multiple full model copies. Each adapter adds only 4-10MB of parameters.

### Memory and compute savings

| Configuration | Trainable Params | Memory for Training |
|---------------|------------------|---------------------|
| Full fine-tuning (350M model) | 350M | ~8-12 GB |
| LoRA r=4 | ~2-4M | ~1-2 GB |
| LoRA r=2 | ~1-2M | ~500MB-1 GB |

**Limitation:** Lorax training speed isn't yet optimized to match Python's PEFT library, and quantized LoRA (QLoRA) isn't implemented.

---

## Building an Elixir-optimized BPE tokenizer

Generic tokenizers fragment Elixir-specific constructs like `|>`, `:ok`, and `@spec` into multiple subwords, wasting context window and losing semantic coherence. A custom tokenizer preserves these as atomic units.

### Tokenizer options in the Elixir ecosystem

The **elixir-nx/tokenizers** library provides Rust NIF bindings to Hugging Face Tokenizers:

```elixir
{:ok, tokenizer} = Tokenizers.Tokenizer.from_pretrained("gpt2")
{:ok, encoding} = Tokenizers.Tokenizer.encode(tokenizer, "def hello, do: :ok")
Tokenizers.Encoding.get_tokens(encoding)
# => ["def", " hello", ",", " do", ":", " :", "ok"]  # Fragmented!
```

For custom training, use **SentencePiece** with user-defined symbols:

```python
import sentencepiece as spm

elixir_symbols = [
    # Operators
    "|>", "->", "<-", "=>", "|", "::", "++", "--", "<>",
    # Atoms
    ":ok", ":error", ":nil", ":true", ":false",
    # Module attributes
    "@spec", "@type", "@doc", "@moduledoc", "@behaviour", "@impl",
    # Keywords
    "defmodule", "def", "defp", "defmacro", "defstruct",
    # Sigils
    "~r", "~w", "~s", "~c", "~R", "~W", "~S", "~C",
    # Control flow
    "do", "end", "fn", "when", "case", "cond", "with",
    # Test constructs
    "describe", "test", "assert", "refute", "assert_raise",
    "property", "check all", "StreamData"
]

spm.SentencePieceTrainer.train(
    input='elixir_corpus.txt',
    model_prefix='elixir_32k',
    vocab_size=32000,
    model_type='bpe',
    character_coverage=1.0,
    user_defined_symbols=','.join(elixir_symbols),
    max_sentencepiece_length=16
)
```

### Vocabulary size recommendations

| Model Focus | Vocab Size | Rationale |
|-------------|------------|-----------|
| Elixir-only | 16,000 | Tight vocabulary, efficient for domain |
| Elixir + OTP/Erlang | 32,000 | Room for Erlang interop patterns |
| Multi-language code | 50,000+ | Matches StarCoder/CodeGen |

For a dedicated Elixir model, **32,000 tokens** balances coverage against embedding matrix size (32K × 768 dims = ~100M parameters just for embeddings at 350M scale).

### Integration with Nx training pipelines

```elixir
defmodule ElixirTokenizer do
  def load! do
    {:ok, tokenizer} = Tokenizers.Tokenizer.from_file("elixir_32k.json")
    tokenizer
  end
  
  def encode(tokenizer, text, opts \\ []) do
    max_length = Keyword.get(opts, :max_length, 2048)
    {:ok, encoding} = Tokenizers.Tokenizer.encode(tokenizer, text)
    
    ids = Tokenizers.Encoding.get_ids(encoding)
    |> Enum.take(max_length)
    |> pad_or_truncate(max_length)
    
    Nx.tensor(ids, type: :s32)
  end
  
  defp pad_or_truncate(ids, max_length) when length(ids) < max_length do
    ids ++ List.duplicate(0, max_length - length(ids))
  end
  defp pad_or_truncate(ids, max_length), do: Enum.take(ids, max_length)
end
```

---

## Transformer architecture in pure Axon

Axon provides composable building blocks for neural networks but lacks built-in multi-head attention. Reference **Bumblebee's model implementations** (GPT-2, LLaMA, Whisper) in `lib/bumblebee/text/` for production-quality transformer code.

### Multi-head attention implementation

```elixir
defmodule Transformer.Attention do
  import Nx.Defn
  
  def multi_head_attention(input, num_heads, head_dim, opts \\ []) do
    hidden_size = num_heads * head_dim
    dropout = Keyword.get(opts, :dropout, 0.1)
    name = Keyword.get(opts, :name, "attention")
    
    # Q, K, V projections
    query = input |> Axon.dense(hidden_size, name: "#{name}.query")
    key = input |> Axon.dense(hidden_size, name: "#{name}.key")
    value = input |> Axon.dense(hidden_size, name: "#{name}.value")
    
    # Custom attention computation
    attention_output = Axon.layer(
      &scaled_dot_product_attention/6,
      [query, key, value],
      num_heads: num_heads,
      head_dim: head_dim,
      name: "#{name}.sdpa"
    )
    
    # Output projection
    attention_output
    |> Axon.dense(hidden_size, name: "#{name}.output")
    |> Axon.dropout(rate: dropout)
  end
  
  defnp scaled_dot_product_attention(query, key, value, opts) do
    num_heads = opts[:num_heads]
    head_dim = opts[:head_dim]
    
    # Reshape for multi-head: [batch, seq, heads, head_dim]
    q = reshape_for_attention(query, num_heads, head_dim)
    k = reshape_for_attention(key, num_heads, head_dim)
    v = reshape_for_attention(value, num_heads, head_dim)
    
    # Scaled dot-product
    scale = Nx.sqrt(Nx.tensor(head_dim, type: Nx.type(query)))
    scores = Nx.dot(q, [3], [0, 1], Nx.transpose(k, axes: [0, 1, 3, 2]), [2], [0, 1])
    scores = scores / scale
    
    # Causal mask for decoder
    weights = Nx.softmax(scores, axis: -1)
    Nx.dot(weights, [3], [0, 1], v, [2], [0, 1])
  end
end
```

### Positional encoding options

**Sinusoidal (fixed)** works well for models under 1B parameters:

```elixir
defmodule Transformer.Positional do
  import Nx.Defn
  
  defn sinusoidal_encoding(max_len, d_model) do
    positions = Nx.iota({max_len, 1})
    dimensions = Nx.iota({1, d_model})
    
    angles = positions / Nx.pow(10000, 2 * Nx.quotient(dimensions, 2) / d_model)
    
    Nx.select(
      Nx.remainder(dimensions, 2) == 0,
      Nx.sin(angles),
      Nx.cos(angles)
    )
  end
end
```

**Rotary Position Embeddings (RoPE)** are supported in Bumblebee for LLaMA-style models and provide better length extrapolation. Reference `Bumblebee.Layers.Transformer` for RoPE implementation patterns.

### Model sizing for 125M-350M parameter range

| Component | 125M Model | 350M Model |
|-----------|-----------|-----------|
| Layers | 12 | 24 |
| Hidden size | 768 | 1024 |
| Attention heads | 12 | 16 |
| FFN intermediate | 3072 | 4096 |
| Sequence length | 2048 | 4096 |
| bf16 weights | ~250 MB | ~700 MB |
| Training memory | ~1-2 GB | ~4-6 GB |

### EXLA compilation and memory optimization

```elixir
# Configure EXLA for GPU training
Nx.default_backend({EXLA.Backend, client: :cuda})
Nx.Defn.default_options(compiler: EXLA)

# Enable gradient checkpointing for memory efficiency
# Nx supports checkpointing in automatic differentiation
model
|> Axon.Loop.trainer(loss, optimizer)
|> Axon.Loop.run(data, %{}, 
    compiler: EXLA,
    # Mixed precision for memory savings
    loss_scale: :dynamic)
```

---

## Pre-training objectives for test-aware models

The pre-training strategy must build both code understanding and test generation capabilities from the start, rather than treating test generation as a downstream fine-tuning task.

### Recommended multi-objective approach

Combine objectives from CodeT5's proven multi-task framework:

**Primary objectives:**

1. **Masked Span Prediction (MSP)**: Core denoising—mask 15% of spans, predict originals
2. **Fill-in-the-Middle (FIM)**: Critical for test completion given code context
3. **Identifier-aware masking**: Mask all occurrences of an identifier, requiring semantic understanding

**Test-specific objectives:**
4. **Assert completion**: Given test setup, predict assertion statements
5. **Test name prediction**: Recover descriptive test name from test body
6. **Code-test bidirectional**: Generate tests from code AND code from tests

```elixir
defmodule PretrainingObjectives do
  def masked_span_prediction(code_tokens, mask_ratio \\ 0.15) do
    # Randomly mask spans of 1-5 tokens
    # Model predicts original tokens
  end
  
  def fill_in_middle(code_tokens) do
    # Split into prefix/middle/suffix
    # Model generates middle given prefix + suffix
    # Critical for: given function signature + test structure, generate assertions
  end
  
  def assert_completion(test_tokens) do
    # Mask assertion expected values
    # Model predicts values from code context
    # Example: assert calculate(5) == [MASK] -> 25
  end
end
```

### Span corruption vs causal LM for test generation

**Encoder-decoder with span corruption** (CodeT5-style) outperforms decoder-only for translation tasks like code→test, providing bidirectional understanding of input code before generating tests. However, decoder-only with FIM enables efficient interactive completion.

**Recommendation**: Encoder-decoder architecture for batch test generation during training, with FIM objective enabling completion-style inference.

### Ontology-augmented pre-training with elixir-core.ttl

Integrate structured knowledge from TTL ontologies by linearizing to text prefixes:

```elixir
# Transform TTL triples to training context
# elixir-core.ttl might contain:
# :GenServer a :OTPBehaviour ; :hasCallback :handle_call, :handle_cast .

defmodule OntologyPrefix do
  def generate_prefix(module_type) when module_type == :genserver do
    """
    [ONTOLOGY] GenServer is an OTP behaviour with callbacks: 
    handle_call/3 (synchronous), handle_cast/2 (async), init/1 (setup).
    [/ONTOLOGY]
    """
  end
end
```

This contextualizes generation—when generating tests for a GenServer, the model understands expected callback patterns and can generate appropriate test structures.

---

## Training infrastructure in Elixir

### Axon.Loop for training orchestration

```elixir
defmodule CodeTestTrainer do
  def train(model, train_data, validation_data, opts \\ []) do
    epochs = Keyword.get(opts, :epochs, 10)
    learning_rate = Keyword.get(opts, :learning_rate, 1.0e-4)
    
    model
    |> Axon.Loop.trainer(
      &combined_loss/2,
      Polaris.Optimizers.adamw(learning_rate: learning_rate)
    )
    |> Axon.Loop.metric(:accuracy)
    |> Axon.Loop.validate(model, validation_data)
    |> Axon.Loop.checkpoint(
      event: :epoch_completed,
      path: "checkpoints/",
      criteria: "validation_loss",
      mode: :min
    )
    |> Axon.Loop.reduce_lr_on_plateau("validation_loss", 
        patience: 3, 
        factor: 0.5)
    |> Axon.Loop.early_stop("validation_loss", patience: 5)
    |> Axon.Loop.run(train_data, %{}, 
        epochs: epochs, 
        compiler: EXLA)
  end
  
  defp combined_loss(predictions, targets) do
    # Multi-task loss combining code and test generation
    mlm_loss = Axon.Losses.categorical_cross_entropy(
      predictions.mlm, targets.mlm)
    test_loss = Axon.Losses.categorical_cross_entropy(
      predictions.test, targets.test)
    
    Nx.add(Nx.multiply(mlm_loss, 0.5), Nx.multiply(test_loss, 0.5))
  end
end
```

### Distributed training across GPUs

```elixir
# Multi-GPU configuration
config :nx, default_backend: {EXLA.Backend, client: :cuda}

# Nx.Serving with partitioning for multi-GPU
children = [
  {Nx.Serving,
    serving: training_serving,
    name: TrainingServing,
    batch_size: 32,
    partitions: true}  # Automatically distributes across available GPUs
]
```

For larger models, Nx supports gradient checkpointing (trading compute for memory) and mixed precision via `:bf16` tensor types and dynamic loss scaling.

### Data loading and batching

```elixir
defmodule CodeTestDataLoader do
  def training_stream(data_path, batch_size, tokenizer) do
    data_path
    |> File.stream!()
    |> Stream.map(&Jason.decode!/1)
    |> Stream.map(&tokenize_pair(&1, tokenizer))
    |> Stream.chunk_every(batch_size)
    |> Stream.map(&collate_batch/1)
  end
  
  defp tokenize_pair(%{"code" => code, "test" => test}, tokenizer) do
    %{
      code_ids: ElixirTokenizer.encode(tokenizer, code),
      test_ids: ElixirTokenizer.encode(tokenizer, test)
    }
  end
  
  defp collate_batch(examples) do
    {
      Nx.stack(Enum.map(examples, & &1.code_ids)),
      Nx.stack(Enum.map(examples, & &1.test_ids))
    }
  end
end
```

---

## Execution feedback loop with Muzak

Integrating mutation testing into the training loop provides execution-grounded rewards that pure language modeling objectives miss. A test that passes doesn't prove it catches bugs—but a test that kills mutations does.

### Reward computation from mutation scores

```elixir
defmodule MuzakReward do
  @doc """
  Compute reward signal from Muzak mutation testing results.
  Higher mutation kill rate = higher reward.
  """
  def compute_reward(generated_test, target_module) do
    # Write generated test to temp file
    test_path = write_temp_test(generated_test, target_module)
    
    # Run Muzak on target module with generated test
    {output, exit_code} = System.cmd("mix", [
      "muzak",
      "--mutations", "50",
      "--only", target_module
    ], env: [{"MIX_ENV", "test"}])
    
    # Parse mutation score
    case parse_mutation_score(output) do
      {:ok, %{caught: caught, total: total}} ->
        # Reward = mutation kill rate
        caught / max(total, 1)
      :error ->
        # Test failed to compile/run = zero reward
        0.0
    end
  end
  
  defp parse_mutation_score(output) do
    # Parse: "50 mutations run - 35 mutations survived"
    regex = ~r/(\d+) mutations run - (\d+) mutations survived/
    case Regex.run(regex, output) do
      [_, total, survived] ->
        total = String.to_integer(total)
        survived = String.to_integer(survived)
        {:ok, %{caught: total - survived, total: total}}
      nil ->
        :error
    end
  end
end
```

### Reinforcement learning integration

Following CodeRL's approach, use mutation rewards for policy gradient updates:

```elixir
defmodule RLTrainer do
  def training_step(model, batch, base_params) do
    # Generate test candidates
    candidates = generate_candidates(model, batch.code, num_samples: 4)
    
    # Score each candidate with Muzak
    rewards = candidates
    |> Enum.map(&MuzakReward.compute_reward(&1, batch.module_path))
    |> Nx.tensor()
    
    # Normalize rewards (baseline subtraction)
    mean_reward = Nx.mean(rewards)
    normalized = Nx.subtract(rewards, mean_reward)
    
    # Policy gradient update
    # ∇θ J(θ) ≈ Σ (reward - baseline) * ∇θ log π(test|code)
    update_policy(model, candidates, normalized, base_params)
  end
end
```

### Caching and optimization strategies

Muzak execution is expensive. Implement aggressive caching:

```elixir
defmodule MuzakCache do
  use GenServer
  
  def get_or_compute(code_hash, test_hash, compute_fn) do
    cache_key = {code_hash, test_hash}
    
    case :ets.lookup(:muzak_cache, cache_key) do
      [{^cache_key, result}] -> result
      [] ->
        result = compute_fn.()
        :ets.insert(:muzak_cache, {cache_key, result})
        result
    end
  end
end
```

**Additional optimizations:**

- Run Muzak asynchronously during training, using cached rewards when fresh results aren't available
- Sample representative mutations rather than running all 1,000 during early training
- Use Muzak Pro's parallel execution when training at scale
- Batch multiple test evaluations against shared mutants

---

## Corpus construction for from-scratch training

Training a code model from scratch requires substantial high-quality data. For Elixir specifically, the available corpus is smaller than Python or JavaScript, making quality filtering critical.

### Minimum corpus size estimates

| Model Size | Minimum Tokens | Realistic Elixir Corpus |
|-----------|---------------|------------------------|
| 125M params | 2-5B tokens | Achievable with augmentation |
| 350M params | 5-20B tokens | Requires Erlang supplement |
| 1B+ params | 50B+ tokens | Not feasible Elixir-only |

### Primary data sources

**Hex.pm packages (~17,000+ packages):**

```elixir
defmodule HexScraper do
  @hex_api "https://hex.pm/api"
  
  def fetch_packages(page \\ 1) do
    {:ok, response} = Req.get("#{@hex_api}/packages", params: [page: page])
    response.body
  end
  
  def download_package(name, version) do
    url = "https://repo.hex.pm/tarballs/#{name}-#{version}.tar"
    {:ok, response} = Req.get(url)
    extract_source(response.body)
  end
end
```

**GitHub Elixir repositories (~25,000-50,000 repos):**

```bash
# GitHub API search for quality Elixir repos
GET https://api.github.com/search/repositories?q=language:elixir+stars:>10&sort=stars
```

### Quality filtering pipeline

```elixir
defmodule QualityFilter do
  @doc "Filter for high-quality code-test pairs"
  def passes_quality_check?(repo) do
    has_tests?(repo) and
    has_documentation?(repo) and
    passes_credo?(repo) and
    recently_maintained?(repo)
  end
  
  defp has_tests?(repo), do: File.exists?(Path.join(repo, "test"))
  defp has_documentation?(repo), do: doc_coverage(repo) > 0.5
  defp passes_credo?(repo), do: credo_score(repo) > 80
  defp recently_maintained?(repo), do: days_since_commit(repo) < 730
end
```

### Code-test pair extraction

```elixir
defmodule PairExtractor do
  def extract_pairs(repo_path) do
    lib_modules = find_modules(Path.join(repo_path, "lib"))
    test_modules = find_modules(Path.join(repo_path, "test"))
    
    for lib_mod <- lib_modules,
        test_mod <- test_modules,
        matches?(lib_mod, test_mod) do
      %{
        module: lib_mod.name,
        code: lib_mod.source,
        tests: test_mod.source,
        docstrings: extract_docs(lib_mod)
      }
    end
  end
  
  defp matches?(lib, test) do
    # Match Foo to FooTest
    String.ends_with?(test.name, "#{lib.name}Test")
  end
end
```

### Balancing test types

Target distribution in training data:

- **60% ExUnit**: Standard `assert`/`refute` patterns
- **25% StreamData**: Property-based tests with `property` and `check all`
- **15% LiveViewTest**: Phoenix LiveView testing patterns

### Deduplication

Apply MinHash-based near-duplicate detection (Jaccard threshold 0.85) after normalization. Expect **~40% reduction** from deduplication based on BigCode findings. Use Google's `deduplicate-text-datasets` tool for efficient processing.

---

## Model architecture for paired code-test generation

### Encoder-decoder vs decoder-only

For generating tests from code, **encoder-decoder** provides natural separation:

- Encoder: Processes input code with bidirectional attention
- Decoder: Generates tests autoregressively with cross-attention to code

**Decoder-only** with FIM works for interactive scenarios where tests are completed inline but requires careful prompt engineering for paired generation.

**Recommendation**: Encoder-decoder (CodeT5-style) for training, with option to distill to decoder-only for deployment.

### Conditioning for different test types

```elixir
defmodule TestGenerationModel do
  def build(opts \\ []) do
    hidden_size = Keyword.get(opts, :hidden_size, 768)
    
    # Shared encoder
    encoder = build_encoder(hidden_size)
    
    # Specialized test heads
    exunit_head = build_decoder(hidden_size, name: "exunit")
    streamdata_head = build_decoder(hidden_size, name: "streamdata")
    liveview_head = build_decoder(hidden_size, name: "liveview")
    
    # Select head based on test type token
    %{
      encoder: encoder,
      heads: %{
        exunit: exunit_head,
        streamdata: streamdata_head,
        liveview: liveview_head
      }
    }
  end
end
```

### Token budget considerations

For Elixir code-test pairs:

- Average module: 200-500 tokens
- Average test file: 300-800 tokens
- With context: 1000-2000 tokens typical

**Sequence length recommendation**: 4096 tokens covers 95%+ of Elixir modules with comprehensive tests. Use sliding window attention if longer context needed.

### Inference optimization

```elixir
defmodule TestInference do
  def generate_tests(serving, code, opts \\ []) do
    test_type = Keyword.get(opts, :test_type, :exunit)
    num_candidates = Keyword.get(opts, :candidates, 4)
    
    # Generate multiple candidates
    candidates = for _ <- 1..num_candidates do
      Nx.Serving.run(serving, %{
        code: code,
        test_type: test_type,
        temperature: 0.3  # Low temperature for correctness
      })
    end
    
    # Rank by syntax validity + optional execution
    candidates
    |> Enum.filter(&valid_elixir_syntax?/1)
    |> Enum.sort_by(&test_quality_score/1, :desc)
    |> List.first()
  end
end
```

For interactive use, deploy with Nx.Serving for automatic batching and GPU utilization:

```elixir
serving = Bumblebee.Text.generation(
  %{model: lora_model, params: merged_params, spec: spec},
  tokenizer,
  generation_config,
  compile: [batch_size: 4, sequence_length: 4096],
  stream: true,
  defn_options: [compiler: EXLA]
)

Kino.start_child({Nx.Serving, name: TestGenerator, serving: serving})
```

---

## Practical implementation roadmap

**Phase 1: Data preparation (2-4 weeks)**

- Scrape Hex.pm packages and GitHub repos
- Train custom 32K BPE tokenizer with Elixir symbols
- Extract and filter code-test pairs
- Target: 5-10GB cleaned corpus

**Phase 2: Pre-training (4-8 weeks)**

- Implement encoder-decoder transformer in Axon
- Train with MSP + FIM + assert completion objectives
- Checkpoint every epoch, validate on held-out packages
- Target: 125M-350M parameter model

**Phase 3: Execution feedback (2-4 weeks)**

- Integrate Muzak reward computation
- Implement RL fine-tuning with policy gradients
- Cache aggressively, sample mutations sparingly

**Phase 4: Task adaptation with Lorax (1-2 weeks)**

- Create ExUnit, StreamData, LiveViewTest adapters
- Fine-tune each adapter on specialized test data
- Validate mutation scores improve per test type

**Phase 5: Deployment**

- Deploy with Nx.Serving in Phoenix application
- Hot-swap Lorax adapters based on test type selection
- Monitor generation quality and mutation scores in production

This architecture enables an Elixir-native approach to test generation that leverages the ecosystem's unique strengths—pattern matching, functional composition, and the BEAM's hot code loading—while providing execution-grounded feedback through Muzak's mutation testing.
